'''
Joshua Meyer

Simulating a Hidden Markov Model

USAGE: $ python q2.py
'''
import numpy as np
import random

class HMM:
    def __init__(self):
        # number of states in model
        self.N = 3
        # number of elements in emissions alphabet
        self.M = 3
        # array of initial state probabilities
        self.PI = np.matrix([0.3, 0.5, 0.2],
                      dtype=float)
        # matrix of state transition probabilities
        self.A = np.matrix([[0.2, 0.6, 0.2],
                           [0.3, 0.3, 0.4],
                           [0.1, 0.8, 0.1]],
                          dtype=float)
        # matrix of emission probabilities
        self.B = np.matrix([[0.3, 0.1, 0.6],
                           [0.5, 0.3, 0.2],
                           [0.1, 0.7, 0.2]],
                          dtype=float)
        # look-up tables for emissions and states
        self.S = ['1','2','3']
        self.K = ['a','b','c']

    
    def generate_string(self):
        pass
    
    def forward_algorithm(self, O):
        '''
        Given some input string O, calculate the probability that
        it was generated by the HMM (specs in __init__).
        p(O|model)

        \Sigma_{X_1 ... X_{T+1}} \pi_{X_i} \prod_{t+1}^{T} a_{X_t X_{t+1}} 
        \cdot b_{X_t,X_t+1} o_t

        '''
        # create matrix to hold local probailities
        Trellis = np.zeros((len(self.S),
                                 len(O)),
                                dtype=float)
        
        # append initial state probs
        Trellis = np.concatenate((self.PI.T,Trellis), axis=1)

        trellisCol=1
        for emission in O:
            emissionIndex = self.K.index(emission)
            for state_i in range(len(self.S)):
                combinedProb = 0
                for state_j in range(len(self.S)):
                    previousProb_j = Trellis[state_j,(trellisCol-1)]
                    transProb = self.A[state_j,state_i]
                    emissionProb = self.B[state_i,emissionIndex]
                    combinedProb += previousProb_j*transProb*emissionProb
                Trellis[state_i,trellisCol] = combinedProb
            trellisCol+=1
        return Trellis

    def most_probable_states(self,Trellis):
        # delete the first entry from start states before printing
        print(np.delete(Trellis.argmax(axis=0),obj=0,axis=1))

        
def cartesian_product(arrays, cartesianProdArray=None):
    """
    Generate a cartesian product of input arrays.

    INPUT
    arrays : list of array-like
        1-D arrays to form the cartesian product of.
    cartesianProdArray : ndarray
        Array to place the cartesian product in.

    OUTPUT
    cartesianProdArray : ndarray
        2-D array of shape (M, len(arrays)) containing cartesian products
        formed of input arrays.

    EXAMPLE
    >>> cartesian(([1], [4, 5], [6, 7]))
    array([[1, 4, 6],
           [1, 4, 7],
           [1, 5, 6],
           [1, 5, 7]])
    """
    # take the input (array_like: lists, tuples, matrices) and convert to array
    arrays = [np.asarray(x) for x in arrays]
    # take the datatype from the first of the arrays
    dtype = arrays[0].dtype

    # find the length of the output array
    n = np.prod([x.size for x in arrays])
    # initialize empty array
    if cartesianProdArray is None:
        cartesianProdArray = np.zeros([n, len(arrays)], dtype=dtype)

    m = n / arrays[0].size
    cartesianProdArray[:,0] = np.repeat(arrays[0], m)
    if arrays[1:]:
        cartesian_product(arrays[1:],
                          cartesianProdArray=cartesianProdArray[0:m,1:])
        for j in range(1, arrays[0].size):
            cartesianProdArray[j*m:(j+1)*m,1:] = cartesianProdArray[0:m,1:]
    return cartesianProdArray


if __name__ == "__main__":
    hmm = HMM()
    allLen3sentences = cartesian_product([hmm.K]*3)
    for sentence in allLen3sentences:
        Trellis = hmm.forward_algorithm(sentence)
        print(sentence)
        hmm.most_probable_states(Trellis)
        print(Trellis)
        print('============')
